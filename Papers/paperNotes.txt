*Shahana& Omman (2014)*
-relationship between gender and writing styles
-features considered:
	*unigrams
	*bigrams
	*word classes
	*POS tags

-preprocessing step of replacing elongated characters with canonical form
	*might actually want to include this as a feature
	*perhaps certain genders are more likely to use


*Schwartz et al (2013)*
-use what people say in social media to find distinctive words, phrases, and topics as functions of known attributes of people (i.e., gender, age, location, or psychological characteristics)

-rather than examining a priori fixed sets of words, use lexicon based on words of txt being analyzed
	*open-vocabulary analysis, specifically differential language analysis (DLA)

-set of correlations between personality/behavior and language available online (could be used as input features for our algorithm?)

-counting word usage over pre-chosen categories of language
	*e.g., “body” lexicon
	*widely used: Linguistic Inquiry and Word Count (LIWC)

-models built from words, phrases, and model-generated topics have higher out-of-sample predictions accuracies than a standard lexicon for each variable of interest (i.e., gender and word use, etc.)

-linguistic feature extraction for DLA
	*a) words and phrases, b) topics
	*words and phrases
		-N-grams (1-3)
		-phrases = sequences of words with high informative value (pointwise mutual information)
		-words and phrases must be used by at least 1% of subjects (focus on common language)
	*topics
		-word clusters created using latent dirichlet allocation (LDA)
		-topics as feature = probability of a subject’s use of each topic

-need large sample sizes to do data-driven DLA over individual words (i.e., more than 500 subjects)


-determined that models using open vocabulary features significantly outperformed those using closed vocabulary features
	*combining the two does not improve over open vocabulary only

-model: SVM


*Bamman et al (2012)* 
-individuals with many same-gender friends tend to use language strongly associated with their gender

-gender assigned to authors using census data (gender of first name = majority count, if occurred more than 1000 times in dataset)

-model: regression
	*features: boolean indicator for appearance of each of the most frequent 10,000 words in dataset (bag of words model)


*Ugheoke thesis*
-gender assigned via names and census data

-features (selected using ranker-filter algorithms in WEKA):
	*k-top words: k most discrimination words used by each set (words most discriminant of male vs. female)
	*k-top stems
	*k-top bigrams/trigrams

-model: SVM


*Burger, Henderson, Kim & Zarrella (2011)*
-used blog profiles associated with twitter profiles to assign gender (184,000 uses labeled with gender)

-females tweet more than males

-users who provide gender on blog profile tweet more than those who don’t

-females more likely to explicitly indicate gender on twitter profile

-features:
	*word-level ngrams
		-count-valued feature functions did not improve performance
		-therefore, just boolean
	*character-level ngrams

-model: balanced Winnow2 (good combo of accuracy, speed, and robustness to irrelevant features)


